{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random search\n",
    "A popular way of tuning the model's hyper-parameters is to conduct a random search. A random search is similar to grid search, with the difference that in a random search you do not cover all possible combinations in the hyper-parameter space. In a random random search only a smaller subset, picked at random, is covered.\n",
    "\n",
    "In this notebook we will review the basic concepts you need to know in order to successfully tune your model using ```RandomizedSearchCV``` from **Sklearn**.\n",
    "\n",
    "We will use a xgboost classifier to demonstrate how to conduct a grid search. However, the technique can be use with any other model. We will start by importing the classifier, the grid search function, and the function that creates the data (dummy data) we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the classifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#Import grid search\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#Import functions to create dummy data\n",
    "from mlb_misc_functions import obtain_train_clf_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Creation\n",
    "We will start by creating the data we will be using in this notebook. We will create two lists (**train_x** and **train_y**) representing the features and the targets already in the format needed for training. In the following block of code these lists are created using the function ```obtain_train_clf_data()```. After that, we print the first 3 elements of each list and the number of elements in each list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the first 3 elements of train_x:\n",
      "[[ -9   2  -3   5   9]\n",
      " [  8   9   4 -10   6]\n",
      " [ -2   5   8   3   1]]\n",
      "the first 3 elements of train_y:\n",
      "[0 1 1]\n",
      "number of elements in the lists\n",
      "8000\n"
     ]
    }
   ],
   "source": [
    "#Create the lists\n",
    "train_x, train_y = obtain_train_clf_data()\n",
    "\n",
    "#Print the first 3 elements of each list, and the number of elements in the lists\n",
    "print(\"the first 3 elements of train_x:\")\n",
    "print(train_x[:3])\n",
    "print(\"the first 3 elements of train_y:\")\n",
    "print(train_y[:3])\n",
    "print(\"number of elements in the lists\")\n",
    "print(len(train_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each list contains 8,000 elements. In the case of **train_x** each element is a list containing 5 numbers, these are the model’s features. On the other hand, the list **train_y** only contains numbers (0 or 1), these are model’s targets\n",
    "\n",
    "# Random Search Implementation\n",
    "The first step is to create the classifier, in this case Xgboost. After that, we define the hyper-parameter space we are going to explore. See the block of code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create classifier\n",
    "xgb_clf = XGBClassifier()\n",
    "\n",
    "#Define hyper-parameter space\n",
    "parameters = {\"n_estimators\": [10, 20, 40, 60, 80, 100, 110, 120, 130, 140], \n",
    "              \"max_depth\": [1, 2, 3, 4, 5],\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the hyper-parameter space is defined using a dictionary. In our case there are 10 possible values for **n_estimators** and 5 possible values for **max_depth**, for a total of 50 possible combinations. However, we will not test all possible combinations.\n",
    "\n",
    "The next step is to create our random search object. This is done by passing the classifier, the dictionary with the hyper parameter space, a method to evaluate the predictions on the test set, and the cross validation splitting strategy to the ```RandomizedSearchCV()``` function. After the object is created, we use the ```fit()``` method to launch our grid search.\n",
    "\n",
    "Before we move on, it is important to mention that we will use the f1 score to evaluate the performance of the classifier. You can use other scoring methods. See [here](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "          estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
       "       n_estimators=100, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1),\n",
       "          fit_params=None, iid='warn', n_iter=10, n_jobs=None,\n",
       "          param_distributions={'n_estimators': [10, 20, 40, 60, 80, 100, 110, 120, 130, 140], 'max_depth': [1, 2, 3, 4, 5]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create the random search object\n",
    "random_search = RandomizedSearchCV(xgb_clf, parameters, n_iter=10, scoring='f1', cv=5, verbose=0)\n",
    "\n",
    "#launch the random search\n",
    "random_search.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to mention that we are using a cross validation of 5 folds (cv=5), and randomly choosing 10 point in our 10x50 hyper-parameter grid. This means that we are doing the following:\n",
    "\n",
    "* Split the data in 5 sets (cv=5), lets call them A, B, C, D, E.\n",
    "* Select at random 10 (out of 50) hyper-parameter combinations.\n",
    "* Pick one combination of hyper-parameters.\n",
    "* Train the classifier using 4 of the sets (let's say A, B, C, D) and obtain scores on the last set (in this case E).\n",
    "* Repeat the last two steps for all possible combinations (9 hyper-parameters, and 5 cv groups).\n",
    "\n",
    "# Results\n",
    "\n",
    "Now we can obtain the best hyper-parameter combination, and the score using ```best_params_``` and ```best_score_```, see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 140, 'max_depth': 3}\n",
      "0.8934610174906333\n"
     ]
    }
   ],
   "source": [
    "print(random_search.best_params_)\n",
    "print(random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final words\n",
    "We went over the basic concepts behind **random search**, by now you should be able to use **RandomizedSearchCV** to tune up the hyper-parameters of your model. It is now your time to start coding! try the following:\n",
    "* Add other hyper-parameters to the dicctionary.\n",
    "* Try a different value for your cross validation splitting strategy (cv = 3, or 6).\n",
    "* Use a different classifier.\n",
    "* Use a different scoring method to meassure the performance of your classifiers.\n",
    "* Try setting verbose=2 when you create the grid search object.\n",
    "\n",
    "More information about the arguments passed to ```RandomizedSearchCV()``` can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
