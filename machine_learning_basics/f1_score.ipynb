{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F<sub>1</sub> Score\n",
    "\n",
    "The F<sub>1</sub> score is a tool used to measure the performance of a classifier. it is defined as:\n",
    "\n",
    "$F_1 = 2\\frac{PR}{P + R}$\n",
    "\n",
    "where $P$ and $R$ are precision and recall, respectively. Therefore, the values of the F<sub>1</sub> score range between 0 and 1. 0 denotes the worst performance, and 1 denotes the perfect classifier.\n",
    "\n",
    "It can also be written in terms of true positives (TP), false positives (FP), and true negatives (TN) see below:\n",
    "\n",
    "$F_1 = \\frac{TP}{TP + 1/2(FP + FN)}$\n",
    "\n",
    "\n",
    "As an example, lets say that you have developed a model that tries to predict if someone is infected with a virus that has spread all around the world (0 for not infected and 1 for infected). In order to measure the performance of your model you find a testing set (a group of people that you know for a fact if they are infected or not) and you use your model to predict who is infected. The results are the following:\n",
    "```\n",
    "Test_set    = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "Predictions = [0, 0, 1, 1, 1, 1, 0, 0, 0, 0]\n",
    "```\n",
    "The modelâ€™s precision and recall are:\n",
    "\n",
    "* $P = 3/(3+1) = 0.75$\n",
    "* $R = 3/(3+2) = 0.6$\n",
    "\n",
    "\n",
    "Therefore, the F<sub>1</sub> score is:\n",
    "* $F_1 = 2\\frac{0.75 \\times 0.6}{0.75 + 0.6} = 0.67$\n",
    "\n",
    "In the block of code below we will show you how to use ```f1_score``` from **Sklearn** to compute the F<sub>1</sub> score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666665\n"
     ]
    }
   ],
   "source": [
    "#Import function\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#Define lists with the targets and the predictions\n",
    "test_set = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "predictions = [0, 0, 1, 1, 1, 1, 0, 0, 0, 0]\n",
    "\n",
    "#Calculate and print the f1\n",
    "f1 = f1_score(test_set, predictions)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final words\n",
    "\n",
    "We went over the basic formula for the F<sub>1</sub> score and how to use  the ```f1_score``` function from **Sklearn** to easily calculate it. For more information about this function (```f1_score```) go [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html).\n",
    "\n",
    "While the F<sub>1</sub> is an useful tool, it is important to note that it gives the same importance to precision and recall. It is also known that it could be missleading when dealing with highly unvalanced set because ignores the true negatives. In order to see this effect, try to calculate the F<sub>1</sub> in the case where the test set is highly unvalanced and we have a dump predictor that always returns 1, see below:\n",
    "```\n",
    "test_set = [1, 1, 1, 1, 1, 1, 0, 1, 0, 1]\n",
    "predictions = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
